{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjm8PPvZ1o76",
        "outputId": "239102d9-58d2-4ffa-d70a-c5e18c3218bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 25798, done.\u001b[K\n",
            "remote: Counting objects: 100% (6376/6376), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1109/1109), done.\u001b[K\n",
            "remote: Total 25798 (delta 5754), reused 5278 (delta 5267), pack-reused 19422 (from 3)\u001b[K\n",
            "Receiving objects: 100% (25798/25798), 134.42 MiB | 12.04 MiB/s, done.\n",
            "Resolving deltas: 100% (22437/22437), done.\n",
            "Updating files: 100% (2498/2498), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-lgwhE1N5_7",
        "outputId": "7e997a20-e2cf-421c-9cec-eb62fa4d290b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cuda_stuff.cuh\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major,\n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        "\n",
        "//MACRO TO DEBUGG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "/** Error checking for use with CUDA Dynamic Parallelism */\n",
        "/*\n",
        "#define cdpErrchk(ans) { cdpAssert((ans), __FILE__, __LINE__); }\n",
        "__device__ void cdpAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      printf(\"GPU kernel assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) assert(0);\n",
        "   }\n",
        "}\n",
        "*/\n",
        "\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iivrxLaYOYPh",
        "outputId": "b8d3507f-b293-4949-cbbf-c0ad9996317d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cuda_stuff.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "# The fmatrix Data Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A97U902HMog4",
        "outputId": "6d92a5ef-7f3a-4f42-8fec-1fea5fb89f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing fmatrix.cuh\n"
          ]
        }
      ],
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include \"cuda_stuff.cuh\" // for IDX2C\n",
        "\n",
        "////////////////////////////////////////\n",
        "// basic data structure and access macro\n",
        "////////////////////////////////////////\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    int cols;\n",
        "    int rows;\n",
        "} fmatrix;\n",
        "\n",
        "/** Access element (i,j) of matrix M\n",
        " *\n",
        " *  Usage example:\n",
        " *  For computing A = B^T + C, loop over i and j with:\n",
        " *    getfm(A,i,j) = getfm(B,j,i) + getfm(C,i,j);\n",
        " **/\n",
        "#define getfm(M,i,j) (M.data[IDX2C(i,j,M.rows)])\n",
        "\n",
        "////////////////////////////////////////\n",
        "// utility functions\n",
        "////////////////////////////////////////\n",
        "/** Returns the number of elements in the matrix.\n",
        " *\n",
        " *  Useful for computing, e.g., the size\n",
        " *  of a 1D-vector that contains the same numbers.\n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "int fmatrix_elements(fmatrix mat);\n",
        "\n",
        "/** Returns the memory occupied by the matrix elements in bytes\n",
        " *  (not including the variables in the struct mat).\n",
        " *\n",
        " *  Useful for allocating memory for the data.\n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "int fmatrix_size(fmatrix mat);\n",
        "\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_assert(fmatrix mat);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Create, copy, destroy\n",
        "////////////////////////////////////////\n",
        "/** Allocate memory on host */\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols);\n",
        "\n",
        "/** Allocate memory on device */\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols);\n",
        "\n",
        "/** Create a matrix representing columns [a,b) of M.\n",
        " *  Note that the new matrix uses a pointer to the\n",
        " *  data of M. The data is not copied to a new location.\n",
        " *  If M is destroyed, this matrix is useless.\n",
        " */\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b);\n",
        "\n",
        "/** Copy data from matrix on device to host\n",
        " *  (no memory allocation). */\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "/** Copy data from matrix on host to device\n",
        " *  (no memory allocation). */\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "/** Copy matrix from device to host, allocating new memory. */\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device);\n",
        "\n",
        "/** Copy matrix from host to device, allocating new memory. */\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host);\n",
        "\n",
        "/** Free data memory on host.\n",
        " *  This zeros out the data pointer of the fmatrix struct,\n",
        " *  so a pointer is required. */\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "\n",
        "/** Free data memory on device.\n",
        " *  This zeros out the data pointer of the fmatrix struct,\n",
        " *  so a pointer is required. */\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Input and Output\n",
        "////////////////////////////////////////\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host.\n",
        " *  If nb<0, print all rows.\n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device.\n",
        " *  If nb<0, print all rows.\n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print a matrix to a csv file.\n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_to_csv(const char* filename, fmatrix mat);\n",
        "\n",
        "/** Read a matrix from a csv file.\n",
        " *\n",
        " *  This version creates the matrix on the host first.\n",
        " */\n",
        "fmatrix fmatrix_device_from_csv(const char* filename);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Useful\n",
        "////////////////////////////////////////\n",
        "\n",
        "/** Create a matrix with random values between -1 and 1\n",
        " *  on the device */\n",
        "fmatrix fmatrix_create_random_on_device(int rows, int cols);\n",
        "\n",
        "#endif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwZ36ifWQ-d",
        "outputId": "8c9d2d30-625f-424b-bcc8-8c5e9aca58d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing fmatrix.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include <curand.h>\n",
        "#include <curand_kernel.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "// for reading CSV files, we use some C++\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "\n",
        "int fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "int fmatrix_size(fmatrix mat) {\n",
        "     return fmatrix_elements(mat) * sizeof(float);\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat));\n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk(\n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat))\n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_device.data, mat_host.data,\n",
        "                   fmatrix_size(mat_host),\n",
        "                   cudaMemcpyHostToDevice\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,\n",
        "                   fmatrix_size(mat_device),\n",
        "                   cudaMemcpyDeviceToHost\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_device);\n",
        "    fmatrix mat_host = fmatrix_create_on_host(mat_device.rows, mat_device.cols);\n",
        "    fmatrix_data_to_host(mat_host,mat_device);\n",
        "    return mat_host;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix mat_device = fmatrix_create_on_device(mat_host.rows, mat_host.cols);\n",
        "    fmatrix_data_to_device(mat_host,mat_device);\n",
        "    return mat_device;\n",
        "}\n",
        "\n",
        "/** We could do it like this, but it would not set our pointer M.data to 0.\n",
        "... fmatrix_free_on_host(M)\n",
        "void fmatrix_free_on_host(fmatrix mat) {\n",
        "    fmatrix_assert(mat);\n",
        "  free(mat.data);\n",
        "  mat.data = 0;\n",
        "  mat.cols = 0;\n",
        "  mat.rows = 0;\n",
        "}\n",
        "*/\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b) {\n",
        "    fmatrix_assert(M);\n",
        "    fmatrix A = {\n",
        "        .data = &getfm(M,0,a),\n",
        "        .cols = b-a,\n",
        "        .rows = M.rows\n",
        "    };\n",
        "    fmatrix_assert(A);\n",
        "    return A;\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "__device__\n",
        "void fmatrix_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "   fmatrix_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n",
        "void fmatrix_device_to_csv(const char* filename, fmatrix mat) {\n",
        "  // Open file\n",
        "  FILE* fp = fopen(filename, \"w\");\n",
        "  // allocate copy\n",
        "  fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "  for (int i = 0 ; i < tmp.rows; i++){\n",
        "    for (int j = 0 ; j<tmp.cols; j++){\n",
        "      // Note: %.15g gives 15 significant digits (full double precision)\n",
        "      fprintf(fp,\"%.15g\", getfm(tmp,i,j));\n",
        "      if (j+1<tmp.cols) {\n",
        "        fprintf(fp,\",\");\n",
        "      }\n",
        "    }\n",
        "    fprintf(fp,\"\\n\");\n",
        "  }\n",
        "  fmatrix_free_on_host(&tmp);\n",
        "  // Close file\n",
        "  fclose(fp);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_create_random_on_device_kernel(fmatrix M) {\n",
        "    // choose a seed (here: the same each launch)\n",
        "    unsigned long seed = 0;\n",
        "    int sequence = 0;\n",
        "    // first, initialize the random numbers\n",
        "    curandState state;\n",
        "    curand_init(seed, sequence, 0, &state);\n",
        "    for (int i = 0; i < fmatrix_elements(M); ++i) {\n",
        "        // curand_uniform creates numbers between 0 and 1\n",
        "        M.data[i] = (curand_uniform(&state)-0.5)*2.0;\n",
        "    }\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_random_on_device(int rows, int cols) {\n",
        "    // Create an uninitialized matrix on the device\n",
        "    fmatrix M = fmatrix_create_on_device(rows,cols);\n",
        "    // Call a kernel with a single thread to fill the values\n",
        "    fmatrix_create_random_on_device_kernel<<<1,1>>>(M);\n",
        "\n",
        "    return M;\n",
        "}\n",
        "\n",
        "/* Count the number of rows and columns in a csv files (without headers) */\n",
        "void count_elements_in_csv(const char* filename, int* rows, int* cols) {\n",
        "  // Note: for the sake of convenience, we use some C++ functions here\n",
        "  using namespace std;\n",
        "\n",
        "  *rows = 0;\n",
        "  *cols = 0;\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "    while (getline(infile, row_as_string, '\\n')) {\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "        int tempcols = 0;\n",
        "        while (getline(line_stream, value, ',')) {\n",
        "          ++tempcols;\n",
        "        }\n",
        "        if (tempcols > *cols) {\n",
        "           *cols = tempcols;\n",
        "        }\n",
        "        ++(*rows);\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n",
        "/** Read the data from a csv file into an fmatrix on the host.\n",
        " *  Careful: We assume that the matrix has the right dimensions!\n",
        " *  Use count_elements_in_csv(...) to get the dimensions if\n",
        " *  unknown.\n",
        " */\n",
        "void fmatrix_fill_from_csv(fmatrix h_M,const char* filename) {\n",
        "  // Note: for the sake of convenience, we use some C++ functions here\n",
        "  using namespace std;\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  int row = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "    while (getline(infile, row_as_string, '\\n')) {\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "        int col = 0;\n",
        "        while (getline(line_stream, value, ',')) {\n",
        "\t\t\t\t\tgetfm(h_M,row,col) = strtod(value.c_str(), NULL);\n",
        "          ++col;\n",
        "\t\t\t\t}\n",
        "        ++row;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_device_from_csv(const char* filename) {\n",
        "  // first read the file to count the number of elements\n",
        "  int rows = 0;\n",
        "  int cols = 0;\n",
        "  count_elements_in_csv(filename,&rows,&cols);\n",
        "\n",
        "  // allocate the matrix on the host\n",
        "  fmatrix h_M = fmatrix_create_on_host(rows,cols);\n",
        "\n",
        "  // read the data into the host matrix\n",
        "  fmatrix_fill_from_csv(h_M,filename);\n",
        "\n",
        "  // copy the matrix to the device\n",
        "  fmatrix M = fmatrix_copy_to_device(h_M);\n",
        "\n",
        "  // destroy the host matrix\n",
        "  fmatrix_free_on_host(&h_M);\n",
        "\n",
        "  return M;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cGkZv_22K3n"
      },
      "source": [
        "# Fmatrix operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxqVkx2evD-W"
      },
      "source": [
        "## Simple Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qYsuM7ivNxy",
        "outputId": "2726d97d-1f88-4b55-aa43-0a2859361478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing fmatrix_simple_softmax.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile fmatrix_simple_softmax.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__\n",
        "void fmatrix_simple_softmax_kernel(fmatrix Z) {\n",
        "    // Each thread works one column of Z\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < Z.cols) {\n",
        "        float sum = 0.0;\n",
        "        for (int k = 0; k < Z.rows; k++) {\n",
        "            getfm(Z, k, idx) = exp(getfm(Z, k, idx));\n",
        "            sum += getfm(Z, k, idx);\n",
        "        }\n",
        "\n",
        "        for (int k = 0; k < Z.rows; k++) {\n",
        "            getfm(Z, k, idx) /= sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_simple_softmax(fmatrix Z) {\n",
        "    fmatrix_assert(Z);\n",
        "\n",
        "    int threadsPerBlock = Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_simple_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(Z);\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "    device_synchronize();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1wBvasltuah"
      },
      "source": [
        "## Stable Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JGBCdsxyaRf",
        "outputId": "ce24a199-8846-4a5a-ad5b-b10b746425c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing fmatrix_stable_softmax.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile fmatrix_stable_softmax.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__\n",
        "void fmatrix_stable_softmax_kernel(fmatrix Z) {\n",
        "    // Each thread processes one column of Z\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (col < Z.cols) {\n",
        "        float max_val = getfm(Z, 0, col);\n",
        "        for (int row = 1; row < Z.rows; row++) {\n",
        "            float val = getfm(Z, row, col);\n",
        "            if (val > max_val) {\n",
        "                max_val = val;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int row = 0; row < Z.rows; row++) {\n",
        "            float exp_val = expf(getfm(Z, row, col) - max_val);\n",
        "            getfm(Z, row, col) = exp_val;\n",
        "            sum += exp_val;\n",
        "        }\n",
        "\n",
        "        for (int row = 0; row < Z.rows; row++) {\n",
        "            getfm(Z, row, col) /= sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_stable_softmax(fmatrix Z) {\n",
        "    fmatrix_assert(Z);\n",
        "\n",
        "    int threadsPerBlock = Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_stable_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(Z);\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "    device_synchronize();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldj59lHN1hjR"
      },
      "source": [
        "## Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFXOEDWs1kZQ",
        "outputId": "122930cd-58c2-488f-e4b5-0525ae59431f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting fmatrix_normalization.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile fmatrix_normalization.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__\n",
        "void compute_mu_sigma_kernel(fmatrix X, fmatrix mu, fmatrix sigma) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < X.rows) {\n",
        "        // Step 1: Compute mean\n",
        "        float sum = 0.0f;\n",
        "        for (int col = 0; col < X.cols; col++) {\n",
        "            sum += getfm(X, row, col);\n",
        "        }\n",
        "        float mean = sum / X.cols;\n",
        "        getfm(mu, row, 0) = mean; // Store mean\n",
        "\n",
        "        // Step 2: Compute standard deviation\n",
        "        float sum_squared_diff = 0.0f;\n",
        "        for (int col = 0; col < X.cols; col++) {\n",
        "            float diff = getfm(X, row, col) - mean;\n",
        "            sum_squared_diff += diff * diff;\n",
        "        }\n",
        "        float std_dev = sqrtf(sum_squared_diff / X.cols);\n",
        "        getfm(sigma, row, 0) = std_dev; // Store standard deviation\n",
        "    }\n",
        "}\n",
        "\n",
        "void compute_mu_sigma(fmatrix X, fmatrix mu, fmatrix sigma) {\n",
        "    fmatrix_assert(X);\n",
        "    fmatrix_assert(mu);\n",
        "    fmatrix_assert(sigma);\n",
        "\n",
        "    int threadsPerBlock = X.rows;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK) {\n",
        "        blocksPerGrid = (threadsPerBlock - 1) / THREADS_PER_BLOCK + 1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    compute_mu_sigma_kernel<<< blocksPerGrid, threadsPerBlock >>>(X, mu, sigma);\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_normalize_kernel(fmatrix X, fmatrix mu, fmatrix sigma) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < X.rows) {\n",
        "        float mean = getfm(mu, row, 0);\n",
        "        float std_dev = getfm(sigma, row, 0);\n",
        "\n",
        "        for (int col = 0; col < X.cols; col++) {\n",
        "            const float epsilon = 1e-8f; // Small constant to prevent division by zero\n",
        "            getfm(X, row, col) = (getfm(X, row, col) - mean) / (std_dev + epsilon);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_normalize(fmatrix X, fmatrix mu, fmatrix sigma) {\n",
        "    fmatrix_assert(X);\n",
        "    fmatrix_assert(mu);\n",
        "    fmatrix_assert(sigma);\n",
        "\n",
        "    int threadsPerBlock = X.rows;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK) {\n",
        "        blocksPerGrid = (threadsPerBlock - 1) / THREADS_PER_BLOCK + 1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_normalize_kernel<<< blocksPerGrid, threadsPerBlock >>>(X, mu, sigma);\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "    device_synchronize();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMIZcyWA2Xcz"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDwkD7ib2gf6"
      },
      "source": [
        "## Simple Softmax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbzNm4Kutmp"
      },
      "source": [
        "### Test extreme values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8xjs_GUuYcl",
        "outputId": "94198a9c-8274-4211-8ede-8db569d6acf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_simple_softmax.cu\"\n",
        "\n",
        "int main() {\n",
        "    fmatrix Z = fmatrix_device_from_csv(\"test_M.csv\");\n",
        "\n",
        "    fmatrix_simple_softmax(Z);\n",
        "    fmatrix_device_to_csv(\"test_Z.csv\", Z);\n",
        "    fmatrix_free_on_device(&Z);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "QpIOSzsQujjq"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 -g -G -I cuda-samples/Common/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYyABBiMuket",
        "outputId": "bbe348ea-0e25-4aee-f68b-c817449d0a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[       nan        nan 0.09003057]\n",
            " [       nan        nan 0.24472848]\n",
            " [       nan        nan 0.66524094]]\n",
            "CPU times: user 5.12 ms, sys: 1.74 ms, total: 6.86 ms\n",
            "Wall time: 205 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "\n",
        "# (column 1: large, column 2: small, column 3: normal)\n",
        "M = np.array([\n",
        "    [1e5, -1e3, 1],\n",
        "    [1e6, -1e4, 2],\n",
        "    [1e7, -1e5, 3],\n",
        "], dtype=np.float32)\n",
        "np.savetxt(\"test_M.csv\", M, delimiter=',')\n",
        "\n",
        "!./a.out\n",
        "Z_test = np.loadtxt(\"test_Z.csv\", delimiter=',', ndmin=2)\n",
        "print(Z_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLLqco_3xbl9"
      },
      "source": [
        "As we can see, large and small values in the input matrix result in a `nan` value in the output matrix (after the softmax).\n",
        "\n",
        "This is normal since our softmax implementation suffers from numerical instability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMZdABiunyi"
      },
      "source": [
        "### Test against scipy implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K69UB3XD2tbq",
        "outputId": "736bacee-e030-4299-947d-78c4a777fc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_simple_softmax.cu\"\n",
        "\n",
        "int main() {\n",
        "    fmatrix Z = fmatrix_device_from_csv(\"test_M.csv\");\n",
        "\n",
        "    fmatrix_simple_softmax(Z);\n",
        "    fmatrix_device_to_csv(\"test_Z.csv\", Z);\n",
        "    fmatrix_free_on_device(&Z);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "IcTvt47V2diK"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 -g -G -I cuda-samples/Common/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V1Y0SZh4IG2",
        "outputId": "ade861c1-e761-4805-9973-a69d2d89baee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random  50  test passed.\n",
            "CPU times: user 233 ms, sys: 48.4 ms, total: 282 ms\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from random import randint\n",
        "\n",
        "nb_tests = 50\n",
        "max_dimension = 5\n",
        "\n",
        "for i in range(nb_tests):\n",
        "  n = randint(1,max_dimension)\n",
        "  m = randint(1,max_dimension)\n",
        "  M = np.random.rand(n,m)\n",
        "  np.savetxt(\"test_M.csv\", M, delimiter=',')\n",
        "\n",
        "  !./a.out\n",
        "  Z_test = np.loadtxt(\"test_Z.csv\", delimiter=',', ndmin=2)\n",
        "  Z = softmax(M, axis=0)\n",
        "  result = np.allclose(Z, Z_test)\n",
        "\n",
        "  if not result:\n",
        "    print(\"Wrong test result:\")\n",
        "    print(\"Original:\\n\", M)\n",
        "    print(\"Expected:\\n\", Z)\n",
        "    print(\"Got wrong result:\\n\", Z_test)\n",
        "    break\n",
        "\n",
        "if result:\n",
        "  print(\"All random \", nb_tests, \" test passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inJw8ZZDyKD9"
      },
      "source": [
        "Our implementation is correct: all the test (in a setting with stable numerical values) are passed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhnttZNotohI"
      },
      "source": [
        "## Stable Softmax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcWiJukRz3kI"
      },
      "source": [
        "### Test extreme values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3rZbqLlz0fq",
        "outputId": "563fc9ff-ed8b-41b4-e60d-3c12b9f37e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_stable_softmax.cu\"\n",
        "\n",
        "int main() {\n",
        "    fmatrix Z = fmatrix_device_from_csv(\"test_M.csv\");\n",
        "\n",
        "    fmatrix_stable_softmax(Z);\n",
        "    fmatrix_device_to_csv(\"test_Z.csv\", Z);\n",
        "    fmatrix_free_on_device(&Z);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Yjh9yJ9fz0fq"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 -g -G -I cuda-samples/Common/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGiP0589z0fr",
        "outputId": "5ac9e508-1a4c-4e0a-9e80-35c061d88fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         1.         0.09003057]\n",
            " [0.         0.         0.24472848]\n",
            " [1.         0.         0.66524088]]\n",
            "CPU times: user 5.48 ms, sys: 1.23 ms, total: 6.71 ms\n",
            "Wall time: 206 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "\n",
        "# (column 1: large, column 2: small, column 3: normal)\n",
        "M = np.array([\n",
        "    [1e5, -1e3, 1],\n",
        "    [1e6, -1e4, 2],\n",
        "    [1e7, -1e5, 3],\n",
        "], dtype=np.float32)\n",
        "np.savetxt(\"test_M.csv\", M, delimiter=',')\n",
        "\n",
        "!./a.out\n",
        "Z_test = np.loadtxt(\"test_Z.csv\", delimiter=',', ndmin=2)\n",
        "print(Z_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI9BUFQ50QzJ"
      },
      "source": [
        "As we can see, the problem of having large and small values is fixed.\n",
        "\n",
        "With the stable implementation we now correctly compute the softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YhOoqi3z_IN"
      },
      "source": [
        "### Test against scipy implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRE52fHrtlqB",
        "outputId": "762f7fc0-4637-40e0-f1d8-ba4b2bedb962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_stable_softmax.cu\"\n",
        "\n",
        "int main() {\n",
        "    fmatrix Z = fmatrix_device_from_csv(\"test_M.csv\");\n",
        "\n",
        "    fmatrix_stable_softmax(Z);\n",
        "    fmatrix_device_to_csv(\"test_Z.csv\", Z);\n",
        "    fmatrix_free_on_device(&Z);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Vfx1M7aQtlqB"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 -g -G -I cuda-samples/Common/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGL3T1gRtlqB",
        "outputId": "0412bfd3-6d1d-4559-9b8b-b12b126b8293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random  50  test passed.\n",
            "CPU times: user 225 ms, sys: 53.3 ms, total: 278 ms\n",
            "Wall time: 10.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from random import randint\n",
        "\n",
        "nb_tests = 50\n",
        "max_dimension = 5\n",
        "\n",
        "for i in range(nb_tests):\n",
        "  n = randint(1,max_dimension)\n",
        "  m = randint(1,max_dimension)\n",
        "  M = np.random.rand(n,m)\n",
        "  np.savetxt(\"test_M.csv\", M, delimiter=',')\n",
        "\n",
        "  !./a.out\n",
        "  Z_test = np.loadtxt(\"test_Z.csv\", delimiter=',', ndmin=2)\n",
        "  Z = softmax(M, axis=0)\n",
        "  result = np.allclose(Z, Z_test)\n",
        "\n",
        "  if not result:\n",
        "    print(\"Wrong test result:\")\n",
        "    print(\"Original:\\n\", M)\n",
        "    print(\"Expected:\\n\", Z)\n",
        "    print(\"Got wrong result:\\n\", Z_test)\n",
        "    break\n",
        "\n",
        "if result:\n",
        "  print(\"All random \", nb_tests, \" test passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I7o7dj_1Uix"
      },
      "source": [
        "Again, even in this case, all test pass. This means that our implementation of the stable softmax is correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbZgAnt67DKK"
      },
      "source": [
        "## Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvwU2kW7H1E"
      },
      "source": [
        "### Test against scipy implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFPrtN587H1E",
        "outputId": "60b0a72d-19e7-4aeb-a885-78c6625e133e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_normalization.cu\"\n",
        "\n",
        "int main() {\n",
        "    fmatrix X = fmatrix_device_from_csv(\"test_M.csv\");\n",
        "    fmatrix mu = fmatrix_create_on_device(X.rows, 1);\n",
        "    fmatrix sigma = fmatrix_create_on_device(X.rows, 1);\n",
        "\n",
        "    compute_mu_sigma(X, mu, sigma);\n",
        "    fmatrix_normalize(X, mu, sigma);\n",
        "\n",
        "    fmatrix_device_to_csv(\"test_X.csv\", X);\n",
        "    fmatrix_device_to_csv(\"test_mu.csv\", mu);\n",
        "    fmatrix_device_to_csv(\"test_sigma.csv\", sigma);\n",
        "\n",
        "    fmatrix_free_on_device(&X);\n",
        "    fmatrix_free_on_device(&mu);\n",
        "    fmatrix_free_on_device(&sigma);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "eZmn0rgA7H1F"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 -g -G -I cuda-samples/Common/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siibdyDq7H1F",
        "outputId": "5d4ef855-424a-4257-9f80-9336b5480a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random  50  test passed.\n",
            "CPU times: user 267 ms, sys: 37.8 ms, total: 305 ms\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from random import randint\n",
        "\n",
        "nb_tests = 50\n",
        "max_dimension = 5\n",
        "\n",
        "for i in range(nb_tests):\n",
        "  n = randint(1,max_dimension)\n",
        "  m = randint(1,max_dimension)\n",
        "  M = np.random.rand(n, m).astype(np.float32)\n",
        "  np.savetxt(\"test_M.csv\", M, delimiter=',')\n",
        "\n",
        "  !./a.out\n",
        "  X_test = np.loadtxt(\"test_X.csv\", delimiter=',', ndmin=2)\n",
        "  mu_test = np.loadtxt(\"test_mu.csv\", delimiter=',', ndmin=2)\n",
        "  sigma_test = np.loadtxt(\"test_sigma.csv\", delimiter=',', ndmin=2)\n",
        "\n",
        "  mu = np.mean(M, axis=1, keepdims=True)\n",
        "  sigma = np.std(M, axis=1, keepdims=True)\n",
        "  epsilon = 1e-8  # Small value to avoid division by zero\n",
        "  X = (M - mu) / (sigma + epsilon)\n",
        "\n",
        "  result = np.allclose(X, X_test, atol=1e-6) and \\\n",
        "          np.allclose(mu, mu_test, atol=1e-6) and \\\n",
        "          np.allclose(sigma, sigma_test, atol=1e-6)\n",
        "\n",
        "  if not result:\n",
        "    print(\"Wrong test result:\")\n",
        "    print(\"Original:\\n\", M)\n",
        "    print(\"Expected:\\n\", X)\n",
        "    print(\"Got wrong result:\\n\", X_test)\n",
        "    print(\"Expected mu:\\n\", mu)\n",
        "    print(\"Got wrong result mu:\\n\", mu_test)\n",
        "    print(\"Expected sigma:\\n\", sigma)\n",
        "    print(\"Got wrong result sigma:\\n\", sigma_test)\n",
        "    break\n",
        "\n",
        "if result:\n",
        "  print(\"All random \", nb_tests, \" test passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqkTIPB7H1F"
      },
      "source": [
        "All test passed: our implementation of the z-normalization is correct.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Po-TEvrWMJ_a"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
